{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a65004f6d2bbdaa739445b17f1d1c4cf",
     "grade": false,
     "grade_id": "cell-ff8c490d066d036c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ed9f481107ab161acfd19f22351c6e2",
     "grade": false,
     "grade_id": "cell-0f5ec53dfcb04ecc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Denoising autoencoders**: We will evaluate denoising autoencoders applied to the MNIST dataset.\n",
    "\n",
    "* Obtain (or write! but this isn't required) a pytorch/tensorflow/etc. code for a denoising autoencoder. Train this autoencoder on the MNIST dataset. Use only the MNIST training set. You should use at least three layers in the encoder and in the decoder.\n",
    "* We now need to determine how well this autoencoder works. For each image in the MNIST test dataset, compute the residual error of the autoencoder. This is the difference between the true image and the reconstruction of that image by the autoencoder. It is an image itself. Prepare a figure showing the mean residual error, and the first five principal components. Each is an image. You should preserve signs (i.e. the mean residual error may have negative as well as positive entries). The way to show these images most informatively is to use a mid gray value for zero, then darker values for more negative image values and lighter values for more positive values. The scale you choose matters. You should show\n",
    "    * mean and five principal components on the same gray scale for all six images, chosen so the largest absolute value over all six images is full dark or full light respectively and\n",
    "    * mean and five principal components on a scale where the gray scale is chosen for each image separately.\n",
    "\n",
    "**Variational autoencoders**: We will evaluate variational autoencoders applied to the MNIST dataset.\n",
    "  * Obtain (or write! but this isn't required) a pytorch/tensorflow/etc. code for a variational autoencoder. Train this autoencoder on the MNIST dataset. Use only the MNIST training set.\n",
    "  * We now need to determine how well the codes produced by this autoencoder can be interpolated.\n",
    "    * For 10 pairs of MNIST test images of the same digit, selected at random, compute the code for each image of the pair. Now compute 7 evenly spaced linear interpolates between these codes, and decode the result into images. Prepare a figure showing this interpolate. Lay out the figure so each interpolate is a row. On the left of the row is the first test image; then the interpolate closest to it; etc; to the last test image. You should have a 10 rows and 9 columns of images.\n",
    "    * For 10 pairs of MNIST test images of different digits, selected at random, compute the code for each image of the pair. Now compute 7 evenly spaced linear interpolates between these codes, and decode the result into images. Prepare a figure showing this interpolate. Lay out the figure so each interpolate is a row. On the left of the row is the first test image; then the interpolate closest to it; etc; to the last test image. You should have a 10 rows and 9 columns of images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9bbd34c7e780ce2647a8da9163af718c",
     "grade": false,
     "grade_id": "cell-be4b877294993f41",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Hints and References**: For the denoising autoencoder, there is an abundance of code online should you choose to obtain one. It may be a good practice to also implement this part from scratch and test what you learned in the CNN assignment. All you have to do is define a network with two groups of layers:\n",
    "  * *Encoder Layers*: This part must take an image and produce a low-dimensional \"code\" of the image. Therefore, the architecture of the netwok must be narrowing down. Let's call this function $f^{\\text{encoder}}$.\n",
    "  * *Decoder Layers*: This part must take a low-dimensional \"code\" of the image and produce the original image. Therefore, the architecture of the netwok must be expanding. Let's call this function $f^{\\text{decoder}}$.\n",
    "  \n",
    "All you have to do is to try and write some code to minimize the following loss:\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N \\|x_i - f^{\\text{decoder}}(f^{\\text{encoder}}(x_i))\\|_2^2$$\n",
    "\n",
    "You may pick any architecture that works as long as it has three layes. The MNIST data has 784 pixels. Therefore, a fully connected network which takes 784 reshaped dimensions to $h_1$ dimensions, then to $h_2$ dimensions, and finally to $h_3$ dimensions is an excellent starting point for an encoder. A vast range of choices can work for these three numbers, but just to give you an idea about their plausible range of values, $h_1$ could be in the order of hundreds, $h_2$ could be in order of tens (or at most a few hundreds), and $h_3$ is supposed to be a low-dimension (preferrably under 10 or at most 20).\n",
    "\n",
    "You can reverse the encoder architecture, to obtain a decoder, and then stack an SGD optimizer on top with default hyper-parameters to train your denoising autoencoder. You must be familiar with the rest of the concepts from earlier assignments such as multi-dimensional scalings and PCA. You also would need to write some basic code to visualize using matplotlib, PIL, etc.\n",
    "\n",
    "For VAEs, you may also be able to implement everything from scratch once you review the material. However, there are a lot of resources and examples for implementing VAEs, and here we share a few of them:\n",
    "\n",
    "  1. Pytorch Tutorials has an example for training VAEs at https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/variational_autoencoder/main.py\n",
    "  2. Another pytorch example for VAEs can be found at https://github.com/pytorch/examples/blob/master/vae/main.py\n",
    "  3. Pyro is a library for bayesian optimization and is based on pytorch, which has a detailed tutorial on how to train VAEs with some high-level story of the math involved https://pyro.ai/examples/vae.html\n",
    "  4. BoTorch is another bayesian optimization library based based on pytorch and has some tutorials for implementing VAEs https://botorch.org/tutorials/vae_mnist\n",
    "  5. If you're a tensorflow fan, you may find some tutorial at https://www.tensorflow.org/probability/examples/Probabilistic_Layers_VAE or  https://www.tensorflow.org/tutorials/generative/cvae\n",
    "  6. Keras fans can also see https://keras.io/examples/generative/vae/\n",
    "  7. etc.\n",
    "  \n",
    "The MNIST data is provided at `../VAE-lib/data_mnist` so that you could use the `torchvision` API for loading the data just like the previous assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88b70267128002739eb4c352b3ee518e",
     "grade": false,
     "grade_id": "cell-d470720bf71d0e77",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Important Note**: This assignment will not be automatically graded and is optional. Therefore, do not expect meaninful grades to be published upon or after submission. However, please make sure to submit your work if you expect it to be reviewed by the instructors for any reason. We will consider the latest submission of your work. \n",
    "\n",
    "Any work that is not submitted will not be viewed by us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import botorch\n",
    "import pyro\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2c8b01ab30982a66445130a0460367bf",
     "grade": true,
     "grade_id": "cell-b28b3957a99730ca",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root='../VAE-lib/data_mnist', train=True, download=False)\n",
    "testset = torchvision.datasets.MNIST(root='../VAE-lib/data_mnist', train=False, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Noises are added to all trainset images\n",
      "Gaussian Noises are added to all testset images\n"
     ]
    }
   ],
   "source": [
    "def add_noise(img,noise_type=\"gaussian\"):\n",
    "    \n",
    "    row,col = 28,28\n",
    "    imgArray = np.array(img).astype(np.float32)\n",
    "    \n",
    "    if noise_type==\"gaussian\":\n",
    "        mean = 0\n",
    "        var = 10\n",
    "        sigma = var**.5\n",
    "        noise = np.random.normal(mean, sigma, imgArray.shape)\n",
    "        noise = noise.reshape(row,col)\n",
    "        imgArray = imgArray + noise\n",
    "        \n",
    "    return imgArray\n",
    "\n",
    "\n",
    "# Add Gaussian noise to each image in trainset and testset\n",
    "\n",
    "noisy_traindata = np.zeros((60000, 28, 28))\n",
    "for i in range(len(trainset.data)):\n",
    "    noisy_traindata[i] = add_noise(trainset.data[i]) # trainset.data contains images, trainset.targets contains labels\n",
    "print(\"Gaussian Noises are added to all trainset images\")\n",
    "\n",
    "noisy_testdata = np.zeros((10000, 28, 28))\n",
    "for i in range(len(testset.data)):\n",
    "    noisy_testdata[i] = add_noise(testset.data[i])\n",
    "print(\"Gaussian Noises are added to all testset images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Audoencoder Model\n",
    "class denoising_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28,256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256,64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64,8),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    \n",
    "        self.decoder=nn.Sequential(\n",
    "            nn.Linear(8,64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64,256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256,28*28),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        return x\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset that includes both clean and noisy images\n",
    "class noisedDataset(Dataset):\n",
    "    def __init__(self,datasetnoised,datasetclean,labels,transform):\n",
    "        self.noise=datasetnoised\n",
    "        self.clean=datasetclean\n",
    "        self.labels=labels\n",
    "        self.transform=transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.noise)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        xNoise=self.noise[idx]\n",
    "        xClean=self.clean[idx]\n",
    "        y=self.labels[idx]\n",
    "    \n",
    "        if self.transform != None:\n",
    "            xNoise=self.transform(xNoise)\n",
    "            xClean=self.transform(xClean)\n",
    "            \n",
    "        return (xNoise,xClean,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tsfms = transforms.Compose([transforms.ToTensor()])\n",
    "tsfms = None\n",
    "\n",
    "noisy_trainset = noisedDataset(noisy_traindata, trainset.data, trainset.targets, tsfms)\n",
    "noisy_testset = noisedDataset(noisy_testdata, testset.data, testset.targets, tsfms)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "trainloader = DataLoader(noisy_trainset, batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(noisy_testset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch  0\n",
      "Current epoch mean loss:  225.59388709309897\n",
      "Starting epoch  1\n",
      "Current epoch mean loss:  225.57675026041667\n",
      "Starting epoch  2\n",
      "Current epoch mean loss:  225.56827655436197\n",
      "Starting epoch  3\n",
      "Current epoch mean loss:  225.56365685221354\n",
      "Starting epoch  4\n",
      "Current epoch mean loss:  225.56247137044272\n",
      "Starting epoch  5\n",
      "Current epoch mean loss:  225.55727053222657\n",
      "Starting epoch  6\n",
      "Current epoch mean loss:  225.55459405110676\n",
      "Starting epoch  7\n",
      "Current epoch mean loss:  225.55413702799478\n",
      "Starting epoch  8\n",
      "Current epoch mean loss:  225.55385546061197\n",
      "Starting epoch  9\n",
      "Current epoch mean loss:  225.55362561035156\n",
      "Starting epoch  10\n",
      "Current epoch mean loss:  225.55336013183594\n",
      "Starting epoch  11\n",
      "Current epoch mean loss:  225.5531839029948\n",
      "Starting epoch  12\n",
      "Current epoch mean loss:  225.55297276204428\n",
      "Starting epoch  13\n",
      "Current epoch mean loss:  225.550031632487\n",
      "Starting epoch  14\n",
      "Current epoch mean loss:  225.5484129231771\n",
      "Starting epoch  15\n",
      "Current epoch mean loss:  225.5482635904948\n",
      "Starting epoch  16\n",
      "Current epoch mean loss:  225.54805641276042\n",
      "Starting epoch  17\n",
      "Current epoch mean loss:  225.5469644124349\n",
      "Starting epoch  18\n",
      "Current epoch mean loss:  225.54674135742187\n",
      "Starting epoch  19\n",
      "Current epoch mean loss:  225.54225008138022\n",
      "Starting epoch  20\n",
      "Current epoch mean loss:  225.54004840494792\n",
      "Starting epoch  21\n",
      "Current epoch mean loss:  225.53983197428386\n",
      "Starting epoch  22\n",
      "Current epoch mean loss:  225.53964990234374\n",
      "Starting epoch  23\n",
      "Current epoch mean loss:  225.53947120768228\n",
      "Starting epoch  24\n",
      "Current epoch mean loss:  225.53935729166668\n",
      "Starting epoch  25\n",
      "Current epoch mean loss:  225.53923885904948\n",
      "Starting epoch  26\n",
      "Current epoch mean loss:  225.53914376627606\n",
      "Starting epoch  27\n",
      "Current epoch mean loss:  225.53909908854166\n",
      "Starting epoch  28\n",
      "Current epoch mean loss:  225.53906038411458\n",
      "Starting epoch  29\n",
      "Current epoch mean loss:  225.5381388671875\n",
      "Starting epoch  30\n",
      "Current epoch mean loss:  225.53774659830728\n",
      "Starting epoch  31\n",
      "Current epoch mean loss:  225.53769560546874\n",
      "Starting epoch  32\n",
      "Current epoch mean loss:  225.53765317382812\n",
      "Starting epoch  33\n",
      "Current epoch mean loss:  225.5375393717448\n",
      "Starting epoch  34\n",
      "Current epoch mean loss:  225.53744173177083\n",
      "Starting epoch  35\n",
      "Current epoch mean loss:  225.53737532552083\n",
      "Starting epoch  36\n",
      "Current epoch mean loss:  225.5373374593099\n",
      "Starting epoch  37\n",
      "Current epoch mean loss:  225.5372320638021\n",
      "Starting epoch  38\n",
      "Current epoch mean loss:  225.53715396321616\n",
      "Starting epoch  39\n",
      "Current epoch mean loss:  225.53704436848957\n",
      "Starting epoch  40\n",
      "Current epoch mean loss:  225.53693610839844\n",
      "Starting epoch  41\n",
      "Current epoch mean loss:  225.53689510904948\n",
      "Starting epoch  42\n",
      "Current epoch mean loss:  225.53686236979166\n",
      "Starting epoch  43\n",
      "Current epoch mean loss:  225.53683533528647\n",
      "Starting epoch  44\n",
      "Current epoch mean loss:  225.53678896484374\n",
      "Starting epoch  45\n",
      "Current epoch mean loss:  225.5367252278646\n",
      "Starting epoch  46\n",
      "Current epoch mean loss:  225.5367001627604\n",
      "Starting epoch  47\n",
      "Current epoch mean loss:  225.53666080729167\n",
      "Starting epoch  48\n",
      "Current epoch mean loss:  225.53663955078125\n",
      "Starting epoch  49\n",
      "Current epoch mean loss:  225.5365938720703\n",
      "Starting epoch  50\n",
      "Current epoch mean loss:  225.53655167643228\n",
      "Starting epoch  51\n",
      "Current epoch mean loss:  225.5365271809896\n",
      "Starting epoch  52\n",
      "Current epoch mean loss:  225.53651120605468\n",
      "Starting epoch  53\n",
      "Current epoch mean loss:  225.53648147786458\n",
      "Starting epoch  54\n",
      "Current epoch mean loss:  225.53644548339844\n",
      "Starting epoch  55\n",
      "Current epoch mean loss:  225.5364446533203\n",
      "Starting epoch  56\n",
      "Current epoch mean loss:  225.53643298339844\n",
      "Starting epoch  57\n",
      "Current epoch mean loss:  225.53641372070314\n",
      "Starting epoch  58\n",
      "Current epoch mean loss:  225.53638868815105\n",
      "Starting epoch  59\n",
      "Current epoch mean loss:  225.53634037272136\n",
      "Starting epoch  60\n",
      "Current epoch mean loss:  225.53633171386718\n",
      "Starting epoch  61\n",
      "Current epoch mean loss:  225.53631795247395\n",
      "Starting epoch  62\n",
      "Current epoch mean loss:  225.53206158854167\n",
      "Starting epoch  63\n",
      "Current epoch mean loss:  225.5316578531901\n",
      "Starting epoch  64\n",
      "Current epoch mean loss:  225.53162259114583\n",
      "Starting epoch  65\n",
      "Current epoch mean loss:  225.5316086344401\n",
      "Starting epoch  66\n",
      "Current epoch mean loss:  225.53160613606772\n",
      "Starting epoch  67\n",
      "Current epoch mean loss:  225.53158767903645\n",
      "Starting epoch  68\n",
      "Current epoch mean loss:  225.5315786376953\n",
      "Starting epoch  69\n",
      "Current epoch mean loss:  225.53155637207033\n",
      "Starting epoch  70\n",
      "Current epoch mean loss:  225.53154324544272\n",
      "Starting epoch  71\n",
      "Current epoch mean loss:  225.53154990234376\n",
      "Starting epoch  72\n",
      "Current epoch mean loss:  225.5315275716146\n",
      "Starting epoch  73\n",
      "Current epoch mean loss:  225.53152176920574\n",
      "Starting epoch  74\n",
      "Current epoch mean loss:  225.53148631184897\n",
      "Starting epoch  75\n",
      "Current epoch mean loss:  225.5315037516276\n",
      "Starting epoch  76\n",
      "Current epoch mean loss:  225.53147221679689\n",
      "Starting epoch  77\n",
      "Current epoch mean loss:  225.53144642740887\n",
      "Starting epoch  78\n",
      "Current epoch mean loss:  225.53143450520832\n",
      "Starting epoch  79\n",
      "Current epoch mean loss:  225.53142934570312\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "model = denoising_model()\n",
    "criterion=nn.MSELoss()\n",
    "optimizer=optim.SGD(model.parameters(),lr=0.01,weight_decay=1e-5)\n",
    "losslist=list()\n",
    "\n",
    "for epoch in range(80):\n",
    "    print(\"Starting epoch \", epoch)\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, data in enumerate(trainloader,0): \n",
    "        dirty, clean, label = data\n",
    "        dirty=dirty.view(dirty.size(0),-1).type(torch.FloatTensor)\n",
    "        clean=clean.view(clean.size(0),-1).type(torch.FloatTensor)\n",
    "        \n",
    "        # forward pass\n",
    "        output = model(dirty)\n",
    "        loss=criterion(output,clean)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    current_epoch_mean_loss = running_loss/60000\n",
    "    print(\"Current epoch mean loss: \", current_epoch_mean_loss)\n",
    "    losslist.append(current_epoch_mean_loss)\n",
    "                \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fada2a02310>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(losslist)),losslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "illinois_payload": {
   "b64z": "",
   "nb_path": "release/VAE/VAE.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3 (Threads: 2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
